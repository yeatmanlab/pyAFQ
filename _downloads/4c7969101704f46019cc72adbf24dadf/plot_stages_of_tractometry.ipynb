{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Making videos the different stages of tractometry\n\nTwo-dimensional figures of anatomical data are somewhat limited, because of the\ncomplex three-dimensional configuration of the brain. Therefored, dynamic\nvideos of anatomical data are useful for exploring the data, as well as for\ncreating dynamic presentations of the results. This example visualizes various\nstages of tractometry, from preprocessed diffusion data to the final tract\nprofiles. We will use the [Fury](https://fury.gl/) software library to\nvisualize individual frames of the results of each stage, and then create\nvideos of each stage of the process using the Python Image Library (PIL, also\nknown as pillow).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport os.path as op\nimport nibabel as nib\nimport numpy as np\nimport tempfile\n\nfrom dipy.io.streamline import load_trk\nfrom dipy.tracking.streamline import (transform_streamlines,\n                                      set_number_of_points)\nfrom dipy.core.gradients import gradient_table\nfrom dipy.align import resample\n\nfrom fury import actor, window\nfrom fury.actor import colormap_lookup_table\nfrom fury.colormap import create_colormap\nfrom matplotlib.cm import tab20\n\nimport AFQ.data.fetch as afd\nfrom AFQ.viz.utils import gen_color_dict\n\nfrom PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a function that makes videos\nThe PIL library has a function that can be used to create animated GIFs from\na series of images. We will use this function to create videos.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p></p></div>\n This function is not part of the AFQ library, but is included here for\n convenience. It is not necessary to understand this function in order to\n understand the rest of the example. If you are interested in learning more\n about this function, you can read the PIL documentation. The function is\n based on the [PIL.Image.save](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.save)\n function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_video(frames, out):\n    \"\"\"\n    Make a video from a series of frames.\n\n    Parameters\n    ----------\n    frames : list of str\n        A list of file names of the frames to be included in the video.\n\n    out : str\n        The name of the output file. Format is determined by the file\n        extension.\n    \"\"\"\n    video = []\n    for nn in frames:\n        frame = Image.open(nn)\n        video.append(frame)\n\n    # Save the frames as an animated GIF\n    video[0].save(\n        out,\n        save_all=True,\n        append_images=video[1:],\n        duration=300,\n        loop=1)\n\n\ntmp = tempfile.mkdtemp()\nn_frames = 72"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get data from HBN POD2\nWe get the same data that is used in the visualization tutorials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "afd.fetch_hbn_preproc([\"NDARAA948VFH\"])\nstudy_path = afd.fetch_hbn_afq([\"NDARAA948VFH\"])[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the processed dMRI data\nThe HBN POD2 dataset was processed using the ``qsiprep`` pipeline. The\nresults from this processing are stored within a sub-folder of the\nderivatives folder wthin the study folder.\nHere, we will start by visualizing the diffusion data. We read in the\ndiffusion data, as well as the gradient table, using the `nibabel` library.\nWe then extract the b0, b1000, and b2000 volumes from the diffusion data.\nWe will use the `actor.slicer` function from `fury` to visualize these. This\nfunction takes a 3D volume as input and returns a `slicer` actor, which can\nthen be added to a `window.Scene` object. We create a helper function that\nwill create a slicer actor for a given volume and a given slice along the x,\ny, or z dimension. We then call this function three times, once for each of\nthe b0, b1000, and b2000 volumes, and add the resulting slicer actors to a\nscene. We set the camera on the scene to a view that we like, and then we\nrecord the scene into png files and subsequently gif animations. We do this\nfor each of the three volumes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "deriv_path = op.join(\n    study_path, \"derivatives\")\n\nqsiprep_path = op.join(\n    deriv_path,\n    'qsiprep',\n    'sub-NDARAA948VFH',\n    'ses-HBNsiteRU')\n\ndmri_img = nib.load(op.join(\n    qsiprep_path,\n    'dwi',\n    'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi.nii.gz'))\n\ngtab = gradient_table(*[op.join(\n    qsiprep_path,\n    'dwi',\n    f'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi.{ext}') for ext in ['bval', 'bvec']])\n\n\ndmri_data = dmri_img.get_fdata()\n\ndmri_b0 = dmri_data[..., 0]\ndmri_b1000 = dmri_data[..., 1]\ndmri_b2000 = dmri_data[..., 65]\n\n\ndef slice_volume(data, x=None, y=None, z=None):\n    slicer_actors = []\n    slicer_actor_z = actor.slicer(data)\n    if z is not None:\n        slicer_actor_z.display_extent(\n            0, data.shape[0] - 1,\n            0, data.shape[1] - 1,\n            z, z)\n        slicer_actors.append(slicer_actor_z)\n    if y is not None:\n        slicer_actor_y = slicer_actor_z.copy()\n        slicer_actor_y.display_extent(\n            0, data.shape[0] - 1,\n            y, y,\n            0, data.shape[2] - 1)\n        slicer_actors.append(slicer_actor_y)\n    if x is not None:\n        slicer_actor_x = slicer_actor_z.copy()\n        slicer_actor_x.display_extent(\n            x, x,\n            0, data.shape[1] - 1,\n            0, data.shape[2] - 1)\n        slicer_actors.append(slicer_actor_x)\n\n    return slicer_actors\n\n\nslicers_b0 = slice_volume(\n    dmri_b0,\n    x=dmri_b0.shape[0] // 2,\n    y=dmri_b0.shape[1] // 2,\n    z=dmri_b0.shape[-1] // 3)\nslicers_b1000 = slice_volume(\n    dmri_b1000,\n    x=dmri_b0.shape[0] // 2,\n    y=dmri_b0.shape[1] // 2,\n    z=dmri_b0.shape[-1] // 3)\nslicers_b2000 = slice_volume(\n    dmri_b2000,\n    x=dmri_b0.shape[0] // 2,\n    y=dmri_b0.shape[1] // 2,\n    z=dmri_b0.shape[-1] // 3)\n\nfor bval, slicers in zip([0, 1000, 2000],\n                         [slicers_b0, slicers_b1000, slicers_b2000]):\n    scene = window.Scene()\n    for slicer in slicers:\n        scene.add(slicer)\n    scene.set_camera(position=(721.34, 393.48, 97.03),\n                     focal_point=(96.00, 114.00, 96.00),\n                     view_up=(-0.01, 0.02, 1.00))\n\n    scene.background((1, 1, 1))\n    window.record(scene, out_path=f'{tmp}/b{bval}',\n                  size=(2400, 2400),\n                  n_frames=n_frames, path_numbering=True)\n\n    make_video(\n        [f'{tmp}/b{bval}{ii:06d}.png' for ii in range(n_frames)], f'b{bval}.gif')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing whole-brain tractography\nOne of the first steps of the pyAFQ pipeline is to generate whole-brain\ntractography. We will visualize the results of this step. We start by reading\nin the FA image, which is used as a reference for the tractography. We then\nload the whole brain tractography, and transform the coordinates of the\nstreamlines into the coordinate frame of the T1-weighted data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "afq_path = op.join(\n    deriv_path,\n    'afq',\n    'sub-NDARAA948VFH',\n    'ses-HBNsiteRU')\n\nfa_img = nib.load(op.join(afq_path,\n                          'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_model-DKI_FA.nii.gz'))\n\n\nsft_whole_brain = load_trk(op.join(afq_path,\n                                   'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_space-RASMM_model-CSD_desc-prob-afq_tractography.trk'), fa_img)\n\n\nt1w_img = nib.load(op.join(deriv_path,\n                           'qsiprep/sub-NDARAA948VFH/anat/sub-NDARAA948VFH_desc-preproc_T1w.nii.gz'))\nt1w = t1w_img.get_fdata()\nsft_whole_brain.to_rasmm()\nwhole_brain_t1w = transform_streamlines(\n    sft_whole_brain.streamlines,\n    np.linalg.inv(t1w_img.affine))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the streamlines\nThe streamlines are visualized in the context of the T1-weighted data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def lines_as_tubes(sl, line_width, **kwargs):\n    line_actor = actor.line(sl, **kwargs)\n    line_actor.GetProperty().SetRenderLinesAsTubes(1)\n    line_actor.GetProperty().SetLineWidth(line_width)\n    return line_actor\n\n\nwhole_brain_actor = lines_as_tubes(whole_brain_t1w, 2)\nslicers = slice_volume(t1w, y=t1w.shape[1] // 2 - 5, z=t1w.shape[-1] // 3)\n\nscene = window.Scene()\n\nscene.add(whole_brain_actor)\nfor slicer in slicers:\n    scene.add(slicer)\n\nscene.set_camera(position=(721.34, 393.48, 97.03),\n                 focal_point=(96.00, 114.00, 96.00),\n                 view_up=(-0.01, 0.02, 1.00))\n\nscene.background((1, 1, 1))\nwindow.record(scene, out_path=f'{tmp}/whole_brain', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video([f\"{tmp}/whole_brain{ii:06d}.png\" for ii in range(n_frames)],\n           \"whole_brain.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Whole brain with waypoints\nWe can also generate a gif video with the whole brain tractography and the\nwaypoints that are used to define the bundles. We will use the same scene as\nbefore, but we will add the waypoints as contours to the scene.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scene.clear()\nwhole_brain_actor = lines_as_tubes(whole_brain_t1w, 2)\n\nscene.add(whole_brain_actor)\nfor slicer in slicers:\n    scene.add(slicer)\n\nscene.background((1, 1, 1))\n\nwaypoint1 = nib.load(\n    op.join(\n        afq_path,\n        \"ROIs\", \"sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_desc-ROI-ARC_L-1-include.nii.gz\"))\n\nwaypoint2 = nib.load(\n    op.join(\n        afq_path,\n        \"ROIs\", \"sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_desc-ROI-ARC_L-2-include.nii.gz\"))\n\nwaypoint1_xform = resample(waypoint1, t1w_img)\nwaypoint2_xform = resample(waypoint2, t1w_img)\nwaypoint1_data = waypoint1_xform.get_fdata() > 0\nwaypoint2_data = waypoint2_xform.get_fdata() > 0\n\nsurface_color = tab20.colors[0]\n\nwaypoint1_actor = actor.contour_from_roi(waypoint1_data,\n                                         color=surface_color,\n                                         opacity=0.5)\n\nwaypoint2_actor = actor.contour_from_roi(waypoint2_data,\n                                         color=surface_color,\n                                         opacity=0.5)\n\nscene.add(waypoint1_actor)\nscene.add(waypoint2_actor)\n\nwindow.record(scene, out_path=f'{tmp}/whole_brain_with_waypoints', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video([f\"{tmp}/whole_brain_with_waypoints{ii:06d}.png\" for ii in range(n_frames)],\n           \"whole_brain_with_waypoints.gif\")\n\nbundle_path = op.join(afq_path,\n                      'bundles')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the arcuate bundle\nNow visualize only the arcuate bundle that is selected with these waypoints.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fa_img = nib.load(op.join(afq_path,\n                          'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_model-DKI_FA.nii.gz'))\nfa = fa_img.get_fdata()\nsft_arc = load_trk(op.join(bundle_path,\n                           'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_space-RASMM_model-CSD_desc-prob-afq-ARC_L_tractography.trk'), fa_img)\n\nsft_arc.to_rasmm()\narc_t1w = transform_streamlines(sft_arc.streamlines,\n                                np.linalg.inv(t1w_img.affine))\n\n\nbundles = [\n    \"ARC_R\",\n    \"ATR_R\",\n    \"CST_R\",\n    \"IFO_R\",\n    \"ILF_R\",\n    \"SLF_R\",\n    \"UNC_R\",\n    \"CGC_R\",\n    \"Orbital\", \"AntFrontal\", \"SupFrontal\", \"Motor\",\n    \"SupParietal\", \"PostParietal\", \"Temporal\", \"Occipital\",\n    \"CGC_L\",\n    \"UNC_L\",\n    \"SLF_L\",\n    \"ILF_L\",\n    \"IFO_L\",\n    \"CST_L\",\n    \"ATR_L\",\n    \"ARC_L\",\n]\n\ncolor_dict = gen_color_dict(bundles)\n\narc_actor = lines_as_tubes(arc_t1w, 8, colors=color_dict['ARC_L'])\nscene.clear()\n\nscene.add(arc_actor)\nfor slicer in slicers:\n    scene.add(slicer)\n\nscene.add(waypoint1_actor)\nscene.add(waypoint2_actor)\n\nwindow.record(scene, out_path=f'{tmp}/arc1', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video([f\"{tmp}/arc1{ii:06d}.png\" for ii in range(n_frames)], \"arc1.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean bundle\nThe next step in processing would be to clean the bundle by removing\nstreamlines that are outliers. We will visualize the cleaned bundle.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scene.clear()\n\nscene.add(arc_actor)\nfor slicer in slicers:\n    scene.add(slicer)\n\nwindow.record(scene, out_path=f'{tmp}/arc2', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video([f\"{tmp}/arc2{ii:06d}.png\" for ii in range(n_frames)], \"arc2.gif\")\n\nclean_bundles_path = op.join(afq_path,\n                             'clean_bundles')\n\nsft_arc = load_trk(op.join(clean_bundles_path,\n                           'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_space-RASMM_model-CSD_desc-prob-afq-ARC_L_tractography.trk'), fa_img)\n\nsft_arc.to_rasmm()\narc_t1w = transform_streamlines(sft_arc.streamlines,\n                                np.linalg.inv(t1w_img.affine))\n\n\narc_actor = lines_as_tubes(arc_t1w, 8, colors=tab20.colors[18])\nscene.clear()\n\nscene.add(arc_actor)\nfor slicer in slicers:\n    scene.add(slicer)\n\nwindow.record(scene, out_path=f'{tmp}/arc3', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video([f\"{tmp}/arc3{ii:06d}.png\" for ii in range(n_frames)], \"arc3.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the values of tissue properties along the bundle\nWe can also visualize the values of tissue properties along the bundle. Here\nwe will visualize the fractional anisotropy (FA) along the arcuate bundle.\nThis is done by using a colormap to color the streamlines according to the\nvalues of the tissue property, with `fury.colormap.create_colormap`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lut_args = dict(scale_range=(0, 1),\n                hue_range=(1, 0),\n                saturation_range=(0, 1),\n                value_range=(0, 1))\n\narc_actor = lines_as_tubes(arc_t1w, 8,\n                           colors=resample(fa_img, t1w_img).get_fdata(),\n                           lookup_colormap=colormap_lookup_table(**lut_args))\nscene.clear()\n\nscene.add(arc_actor)\nfor slicer in slicers:\n    scene.add(slicer)\n\nwindow.record(scene, out_path=f'{tmp}/arc4', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video([f\"{tmp}/arc4{ii:06d}.png\" for ii in range(n_frames)], \"arc4.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core of the bundle and tract profile\nFinally, we can visualize the core of the bundle and the tract profile. The\ncore of the bundle is the median of the streamlines, and the tract profile is\nthe values of the tissue property along the core of the bundle.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "core_arc = np.median(np.asarray(set_number_of_points(arc_t1w, 20)), axis=0)\n\nfrom dipy.stats.analysis import afq_profile\nsft_arc.to_vox()\narc_profile = afq_profile(fa, sft_arc.streamlines, affine=np.eye(4),\n                          n_points=20)\n\ncore_arc_actor = lines_as_tubes(\n    [core_arc],\n    40,\n    colors=create_colormap(arc_profile, 'viridis')\n)\n\narc_actor = lines_as_tubes(arc_t1w, 1,\n                           colors=resample(fa_img, t1w_img).get_fdata(),\n                           lookup_colormap=colormap_lookup_table(**lut_args))\n\nscene.clear()\n\nfor slicer in slicers:\n    scene.add(slicer)\nscene.add(arc_actor)\nscene.add(core_arc_actor)\n\nwindow.record(scene, out_path=f'{tmp}/arc5', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video([f\"{tmp}/arc5{ii:06d}.png\" for ii in range(n_frames)], \"arc5.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core of all bundles and their tract profiles\nSame as before, but for all bundles.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scene.clear()\n\nfor slicer in slicers:\n    scene.add(slicer)\n\nfor bundle in bundles:\n    sft = load_trk(op.join(clean_bundles_path,\n                           f'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_space-RASMM_model-CSD_desc-prob-afq-{bundle}_tractography.trk'), fa_img)\n\n    sft.to_rasmm()\n    bundle_t1w = transform_streamlines(sft.streamlines,\n                                       np.linalg.inv(t1w_img.affine))\n\n    bundle_actor = lines_as_tubes(bundle_t1w, 8, colors=color_dict[bundle])\n    scene.add(bundle_actor)\n\nwindow.record(scene, out_path=f'{tmp}/all_bundles', size=(2400, 2400),\n              n_frames=n_frames, path_numbering=True)\n\nmake_video(\n    [f\"{tmp}/all_bundles{ii:06d}.png\" for ii in range(n_frames)], \"all_bundles.gif\")\n\n\nscene.clear()\n\nfor slicer in slicers:\n    scene.add(slicer)\n\ntract_profiles = []\nfor bundle in bundles:\n    sft = load_trk(op.join(clean_bundles_path,\n                           f'sub-NDARAA948VFH_ses-HBNsiteRU_acq-64dir_space-T1w_desc-preproc_dwi_space-RASMM_model-CSD_desc-prob-afq-{bundle}_tractography.trk'), fa_img)\n    sft.to_rasmm()\n    bundle_t1w = transform_streamlines(sft.streamlines,\n                                       np.linalg.inv(t1w_img.affine))\n\n    core_bundle = np.median(np.asarray(\n        set_number_of_points(bundle_t1w, 20)), axis=0)\n    sft.to_vox()\n    tract_profiles.append(\n        afq_profile(fa, sft.streamlines, affine=np.eye(4),\n                    n_points=20))\n\n    core_actor = lines_as_tubes(\n        [core_bundle],\n        40,\n        colors=create_colormap(tract_profiles[-1], 'viridis')\n    )\n\n    scene.add(core_actor)\n\nwindow.record(scene,\n              out_path=f'{tmp}/all_tract_profiles',\n              size=(2400, 2400),\n              n_frames=n_frames,\n              path_numbering=True)\n\nmake_video([f\"{tmp}/all_tract_profiles{ii:06d}.png\" for ii in range(n_frames)],\n           \"all_tract_profiles.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tract profiles as a table\nFinally, we can visualize the tract profiles as a table. This is done by\nplotting the tract profiles for each bundle as a line plot, with the x-axis\nrepresenting the position along the bundle, and the y-axis representing the\nvalue of the tissue property. We will use the `matplotlib` library to create\nthis plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor ii, bundle in enumerate(bundles):\n    ax.plot(np.arange(ii * 20, (ii + 1) * 20),\n            tract_profiles[ii],\n            color=color_dict[bundle],\n            linewidth=3)\nax.set_xticks(np.arange(0, 20 * len(bundles), 20))\nax.set_xticklabels(bundles, rotation=45, ha='right')\nfig.set_size_inches(10, 5)\nplt.subplots_adjust(bottom=0.2)\nfig.savefig('tract_profiles_as_table.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}