{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cloudknot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6568daee6127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcloudknot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cloudknot'"
     ]
    }
   ],
   "source": [
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.set_region('us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afq_hcp(params):\n",
    "    subject, hcp_ak, hcp_sk = params\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import s3fs\n",
    "    import json\n",
    "    import logging\n",
    "    import os.path as op\n",
    "    import nibabel as nib\n",
    "    import dipy.data as dpd\n",
    "    import dipy.tracking.utils as dtu\n",
    "    import dipy.tracking.streamline as dts\n",
    "    from dipy.io.streamline import save_tractogram, load_tractogram\n",
    "    from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "    from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "    from dipy.io.stateful_tractogram import Space\n",
    "    import dipy.core.gradients as dpg\n",
    "    from dipy.segment.mask import median_otsu\n",
    "\n",
    "    import AFQ.data as afd\n",
    "    import AFQ.tractography as aft\n",
    "    import AFQ.registration as reg\n",
    "    import AFQ.dti as dti\n",
    "    import AFQ.segmentation as seg\n",
    "    from AFQ import api\n",
    "    from AFQ import csd\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    log = logging.getLogger(__name__)    \n",
    "    \n",
    "    log.info(f\"Fetching HCP subject {subject}\")\n",
    "    afd.fetch_hcp([subject], \n",
    "                  profile_name=False,\n",
    "                  aws_access_key_id=hcp_ak,\n",
    "                  aws_secret_access_key=hcp_sk)    \n",
    "        \n",
    "    dwi_dir = op.join(afd.afq_home, 'HCP', 'derivatives',\n",
    "                      'dmriprep', f'sub-{subject}', 'sess-01/dwi')\n",
    "\n",
    "    anat_dir = op.join(afd.afq_home, 'HCP', 'derivatives',\n",
    "                      'dmriprep', f'sub-{subject}', 'sess-01/anat')\n",
    "\n",
    "    hardi_fdata = op.join(dwi_dir, f\"sub-{subject}_dwi.nii.gz\")\n",
    "    hardi_fbval = op.join(dwi_dir, f\"sub-{subject}_dwi.bval\")\n",
    "    hardi_fbvec = op.join(dwi_dir, f\"sub-{subject}_dwi.bvec\")\n",
    "\n",
    "    log.info(f\"Reading data from file {hardi_fdata}\")\n",
    "    img = nib.load(hardi_fdata)\n",
    "    log.info(f\"Creating gradient table from {hardi_fbval} and {hardi_fbvec}\")\n",
    "    gtab = dpg.gradient_table(hardi_fbval, hardi_fbvec)\n",
    "    \n",
    "    bucket_name = 'hcp.recobundles'\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    wm_mask_fname = f'{bucket_name}/sub-{subject}/sub-{subject}_wm_mask.nii.gz'\n",
    "    if fs.exists(wm_mask_fname):\n",
    "        log.info(f\"WM mask exists. Reading from {wm_mask_fname}\")\n",
    "        wm_img = afd.s3fs_nifti_read(wm_mask_fname)\n",
    "        wm_mask = wm_img.get_data()\n",
    "    else:\n",
    "        log.info(f\"Calculating WM segmentation\")\n",
    "        wm_labels=[250, 251, 252, 253, 254, 255, 41, 2, 16, 77]\n",
    "        seg_img = nib.load(op.join(anat_dir, f\"sub-{subject}_aparc+aseg.nii.gz\"))\n",
    "        seg_data_orig = seg_img.get_fdata()\n",
    "        # For different sets of labels, extract all the voxels that\n",
    "        # have any of these values:\n",
    "        wm_mask = np.sum(np.concatenate([(seg_data_orig == l)[..., None]\n",
    "                                        for l in wm_labels], -1), -1)\n",
    "\n",
    "        # Resample to DWI data:\n",
    "        dwi_data = img.get_fdata()\n",
    "        wm_mask = np.round(reg.resample(wm_mask, \n",
    "                                        dwi_data[..., 0],\n",
    "                                        seg_img.affine,\n",
    "                                        img.affine)).astype(int)\n",
    "\n",
    "        wm_img = nib.Nifti1Image(wm_mask.astype(int),\n",
    "                                 img.affine)\n",
    "        log.info(f\"Saving to {wm_mask_fname}\")\n",
    "        afd.s3fs_nifti_write(wm_img, wm_mask_fname)\n",
    "    \n",
    "    fa_fname = f'{bucket_name}/sub-{subject}/sub-{subject}_dti_FA.nii.gz'\n",
    "    dti_params_fname = f'{bucket_name}/sub-{subject}/sub-{subject}_dti.nii.gz'\n",
    "    dti_meta_fname = f'{bucket_name}/sub-{subject}/sub-{subject}_dti.json'\n",
    "    if fs.exists(fa_fname):\n",
    "        log.info(f\"DTI already exists. Reading FA from {fa_fname}\")\n",
    "        log.info(f\"DTI already exists. Reading params from {dti_params_fname}\")\n",
    "        FA_img = afd.s3fs_nifti_read(fa_fname)\n",
    "        dti_params = afd.s3fs_nifti_read(dti_params_fname)\n",
    "    else:\n",
    "        log.info(\"Calculating DTI\")\n",
    "        dti_params = dti.fit_dti(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                out_dir='.', b0_threshold=50,\n",
    "                                mask=wm_mask)\n",
    "        FA_img = nib.load('./dti_FA.nii.gz')\n",
    "        log.info(f\"Writing FA to {fa_fname}\")\n",
    "        afd.s3fs_nifti_write(FA_img, fa_fname)\n",
    "        dti_params_img = nib.load('./dti_params.nii.gz')\n",
    "        log.info(f\"Writing DTI params to {dti_params_fname}\")\n",
    "        afd.s3fs_nifti_write(dti_params_img, dti_params_fname)\n",
    "        dti_params_json = {\"Model\": \"Diffusion Tensor\",\n",
    "                           \"OrientationRepresentation\": \"param\",\n",
    "                            \"ReferenceAxes\": \"xyz\",\n",
    "                            \"Parameters\": {\n",
    "                                \"FitMethod\": \"ols\",\n",
    "                                \"OutlierRejection\": False\n",
    "                                }\n",
    "                          }\n",
    "        log.info(f\"Writing DTI metadata to {dti_meta_fname}\")\n",
    "        afd.s3fs_json_write(dti_params_json, dti_meta_fname)\n",
    "\n",
    "    log.info(f\"Reading FA data from img\")\n",
    "    FA_data = FA_img.get_fdata()\n",
    "\n",
    "    csd_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_csd.nii.gz'\n",
    "    csd_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_csd.json'\n",
    "\n",
    "    if fs.exists(csd_fname):\n",
    "        log.info(f\"CSD already exists. Getting it from {csd_fname}\")        \n",
    "        csd_params = afd.s3fs_nifti_read(csd_fname)\n",
    "    else:\n",
    "        log.info(f\"Calculating CSD\")        \n",
    "        csd_params = csd.fit_csd(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                 out_dir='.', b0_threshold=50,\n",
    "                                 mask=wm_mask)\n",
    "        afd.s3fs_nifti_write(nib.load(csd_params), csd_fname)\n",
    "\n",
    "        \n",
    "        csd_params_json = {\n",
    "    \"Model\": \"Constrained Spherical Deconvolution (CSD)\",\n",
    "    \"ModelURL\": \"https://github.com/nipy/dipy/commit/abf31d15a0ee5dc0704ee03ebbba57358d540612\",\n",
    "    \"Shells\": [ 0, 1000, 2000, 3000 ],\n",
    "    \"Parameters\": {\n",
    "        \"ResponseFunctionTensor\" : \"auto\",\n",
    "        \"SphericalHarmonicBasis\": \"Descoteaux\",\n",
    "        \"NonNegativityConstraint\": \"hard\",\n",
    "        \"SphericalHarmonicDegree\" : 8\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        log.info(f\"Writing CSD metadata to {csd_meta_fname}\")\n",
    "        afd.s3fs_json_write(csd_params_json, csd_meta_fname)\n",
    "\n",
    "\n",
    "    csd_streamlines_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det.trk'\n",
    "    csd_streamlines_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det.json'\n",
    "    if fs.exists(csd_streamlines_fname):\n",
    "        log.info(f\"Streamlines already exist. Loading from {csd_streamlines_fname}\")        \n",
    "        fs.download(csd_streamlines_fname, './csd_streamlines.trk')\n",
    "        tg = load_tractogram('./csd_streamlines.trk', img)\n",
    "        streamlines = tg.streamlines\n",
    "    else:\n",
    "        log.info(f\"Generating streamlines\")      \n",
    "        seed_roi = np.zeros(img.shape[:-1])\n",
    "        seed_roi[FA_data > 0.4] = 1\n",
    "        seed_roi[wm_mask < 1] = 0\n",
    "        streamlines = aft.track(csd_params, seed_mask=seed_roi,\n",
    "                                directions='det', stop_mask=FA_data,\n",
    "                                stop_threshold=0.1)\n",
    "        log.info(f\"After tracking, there are {len(streamlines)} streamlines\")\n",
    "        sft = StatefulTractogram(streamlines, img, Space.RASMM)\n",
    "        save_tractogram(sft, './csd_streamlines.trk',\n",
    "                        bbox_valid_check=False)\n",
    "        log.info(f\"Uploading streamlines to {csd_streamlines_fname}\")\n",
    "        fs.upload('./csd_streamlines.trk', csd_streamlines_fname)\n",
    "        csd_streamlines_json = {\n",
    "            \"Algorithm\" : \"LocalTracking\",\n",
    "            \"AlgorithmURL\":\"https://github.com/yeatmanlab/pyAFQ/commit/c04835cd4ca13d28c20bb449d6f088e656c55e57\",\n",
    "            \"Parameters\":{\n",
    "            \"SeedRoi\": \"dti_FA>0.4\",\n",
    "            \"Directions\": \"det\",\n",
    "            \"StopMask\" : \"dti_FA<0.1\"}\n",
    "            }\n",
    "        log.info(f\"Writing streamlines metadata to {csd_streamlines_meta_fname}\")\n",
    "        afd.s3fs_json_write(csd_streamlines_json, csd_streamlines_meta_fname)\n",
    "    \n",
    "    streamlines = dts.Streamlines(\n",
    "            dtu.transform_tracking_output(streamlines,\n",
    "                                  np.linalg.inv(img.affine)))\n",
    " \n",
    "    log.info(\"Segmenting\")\n",
    "        \n",
    "    # Use the default for waypoint ROI\n",
    "    bundles = api.make_bundle_dict()\n",
    "\n",
    "    segmentation = seg.Segmentation(b0_threshold=50,\n",
    "                                    prob_threshold=10,\n",
    "                                    return_idx=True)\n",
    "    segmentation.segment(bundles, \n",
    "                         streamlines, \n",
    "                         fdata=hardi_fdata,\n",
    "                         fbval=hardi_fbval,\n",
    "                         fbvec=hardi_fbvec)\n",
    "\n",
    "    fiber_groups = segmentation.fiber_groups\n",
    "\n",
    "    sl_count = []\n",
    "    for kk in fiber_groups:\n",
    "        log.info(f\"Cleaning {kk}\")\n",
    "        len_before = len(fiber_groups[kk]['sl'])\n",
    "        log.info(f\"Before cleaning there are {len_before} streamlines\")\n",
    "        new_fibers, idx_in_bundle = seg.clean_bundle(\n",
    "                            fiber_groups[kk]['sl'],\n",
    "                            return_idx=True, \n",
    "                            clean_threshold=3)\n",
    "\n",
    "        log.info(f\"After cleaning there are {len(new_fibers)} streamlines\")\n",
    "        idx_in_global = fiber_groups[kk]['idx'][idx_in_bundle]\n",
    "        \n",
    "        sl_count.append(len(new_fibers))\n",
    "        log.info(f\"There are {sl_count[-1]} streamlines in {kk}\")\n",
    "        sft = StatefulTractogram(\n",
    "            dtu.transform_tracking_output(new_fibers, img.affine),\n",
    "            img, Space.RASMM)\n",
    "\n",
    "        local_tg_fname = './%s_reco.trk'%kk\n",
    "        save_tractogram(sft, local_tg_fname,\n",
    "                        bbox_valid_check=False)\n",
    "        tg_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-afq_bundle-{kk}.trk'\n",
    "        log.info(f\"Uploading {local_tg_fname} to {tg_fname}\")\n",
    "        fs.upload('./%s_reco.trk'%kk, tg_fname)\n",
    "        tg_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-afq_bundle-{kk}.json'\n",
    "        tg_meta_json = {\n",
    "            \"Algorithm\" : \"AFQ\",\n",
    "            \"AlgorithmURL\" : \"https://github.com/yeatmanlab/pyAFQ/commit/f0f486d\",\n",
    "            \"Parameters\":\n",
    "            {\"clean_threshold\":3,\n",
    "             \"prob_threshold\": 10}\n",
    "        }\n",
    "        \n",
    "        log.info(f\"Uploading segmentation metadata to {tg_meta_fname}\")\n",
    "        afd.s3fs_json_write(tg_meta_json, tg_meta_fname)\n",
    "\n",
    "        np.save('bundle_idx.npy', idx_in_global)\n",
    "        idx_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-afq_bundle-{kk}_idx.npy'\n",
    "        log.info(f\"Uploading bundle indices to {idx_fname}\")\n",
    "        fs.upload('bundle_idx.npy', idx_fname)\n",
    "\n",
    "    log.info(\"Saving streamline counts\")\n",
    "    sl_count = pd.DataFrame(data=sl_count, index=fiber_groups.keys(), columns=[\"streamlines\"])\n",
    "    sl_count.to_csv(\"./sl_count.csv\")\n",
    "    sl_count_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-afq_counts.csv'\n",
    "    log.info(f\"Uploading streamline counts to {sl_count_fname}\")\n",
    "    fs.upload(\"./sl_count.csv\", sl_count_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP = configparser.ConfigParser()\n",
    "CP.read_file(open(op.join(op.expanduser('~'), '.aws', 'credentials')))\n",
    "CP.sections()\n",
    "ak = CP.get('hcp', 'AWS_ACCESS_KEY_ID')\n",
    "sk = CP.get('hcp', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "afq_knot = ck.Knot(name='afq_hcp-64gb-191101-27',\n",
    "                  func=afq_hcp,\n",
    "                  image_github_installs=\"https://github.com/arokem/pyAFQ.git@f0f486d\",\n",
    "                  pars_policies=('AmazonS3FullAccess',),\n",
    "                  resource_type=\"SPOT\",\n",
    "                  bid_percentage=100,\n",
    "                  memory=64000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [(sub, ak, sk) for sub in [\n",
    "            100408,\n",
    "            100307,\n",
    "            100610,\n",
    "            101006,\n",
    "            101107,\n",
    "            101309,\n",
    "            101410,\n",
    "            101915,\n",
    "            102008,\n",
    "            102109,\n",
    "            102311,\n",
    "            102513,\n",
    "            100206,\n",
    "            970764,\n",
    "            971160,\n",
    "            972566,\n",
    "            973770,\n",
    "            978578,\n",
    "            979984,\n",
    "            983773,\n",
    "            984472]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = afq_knot.map(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID              Name                        Status   \n",
      "---------------------------------------------------------\n",
      "a3f9cf77-7504-4c22-a3ba-946a10400867        afq-hcp-64gb-191101-27-0        SUBMITTED\n"
     ]
    }
   ],
   "source": [
    "afq_knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0 = afq_knot.jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'PENDING',\n",
       " 'statusReason': None,\n",
       " 'attempts': [],\n",
       " 'arrayProperties': {'statusSummary': {'STARTING': 1,\n",
       "   'FAILED': 0,\n",
       "   'RUNNING': 18,\n",
       "   'SUCCEEDED': 0,\n",
       "   'RUNNABLE': 2,\n",
       "   'SUBMITTED': 0,\n",
       "   'PENDING': 0},\n",
       "  'size': 21}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j0.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afq_knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}