
The pyAFQ configuration file
----------------------------

This file should be a `toml <https://github.com/toml-lang/toml>`_ file. At
minimum, the file should contain information about the location of the
`dmriprep` folder::

    [files]
    dmriprep_folder = '/path/to/study/derivatives/dmriprep'


But additional configuration options can be provided.
See an example configuration file below::

    title = "My AFQ analysis"

    # Initialize an AFQ object.
    # Some special notes on parameters:
    # In tracking_params, parameters with the suffix mask which are also
    # a mask from AFQ.definitions.mask will be handled automatically by the
    # api. You can set additional parameters for a given step of the process
    # by directly calling the relevant api function. For example,
    # to set the sh_order for csd to 4, call:
    # myafq._csd(sh_order=4)
    # before otherwise generating the csd file.
    
    # Use '' to indicate None
    # Wrap dictionaries in quotes
    # Wrap definition object instantiations in quotes
    
    [BIDS]
    
    # The path to preprocessed diffusion data organized in a BIDS
    # dataset. This should contain a BIDS derivative dataset with
    # preprocessed dwi/bvals/bvecs.
    bids_path = ''
    
    # Filter to pass to bids_layout.get when finding DWI files.
    # Default: {"suffix": "dwi"}
    bids_filters = "{'suffix': 'dwi'}"
    
    # The name of the pipeline used to preprocess the DWI data.
    # Default: "all".
    dmriprep = 'all'
    
    # BIDS filters for inputing a user made tractography file.
    # If None, tractography will be performed automatically.
    # Default: None
    custom_tractography_bids_filters = ''
    
    [REGISTRATION]
    
    # The value of b under which
    # it is considered to be b0. Default: 50.
    b0_threshold = 50
    
    # Whether to use patch2self
    # to denoise the dwi data.
    # Default: False
    patch2self = false
    
    # Whether to use robust_tensor_fitting when
    # doing dti. Only applies to dti.
    # Default: False
    robust_tensor_fitting = false
    
    # Minimum b value you want to use
    # from the dataset (other than b0), inclusive.
    # If None, there is no minimum limit. Default: None
    min_bval = ''
    
    # Maximum b value you want to use
    # from the dataset (other than b0), inclusive.
    # If None, there is no maximum limit. Default: None
    max_bval = ''
    
    # The target image data for registration.
    # Can either be a Nifti1Image, a path to a Nifti1Image, or
    # if "mni_T2", "dti_fa_template", "hcp_atlas", or "mni_T1",
    # image data will be loaded automatically.
    # If "hcp_atlas" is used, slr registration will be used
    # and reg_subject should be "subject_sls".
    # Default: "mni_T1"
    reg_template = 'mni_T1'
    
    # The source image data to be registered.
    # Can either be a Nifti1Image, bids filters for a Nifti1Image, or
    # if "b0", "dti_fa_subject", "subject_sls", or "power_map,"
    # image data will be loaded automatically.
    # If "subject_sls" is used, slr registration will be used
    # and reg_template should be "hcp_atlas".
    # Default: "power_map"
    reg_subject = 'power_map'
    
    # This will be used to create
    # the brain mask, which gets applied before registration to a
    # template.
    # If None, no brain mask will not be applied,
    # and no brain mask will be applied to the template.
    # Default: B0Mask()
    brain_mask = 'B0Mask({})'
    
    # This defines how to either create a mapping from
    # each subject space to template space or load a mapping from
    # another software. If creating a map, will register reg_subject and
    # reg_template.
    # Default: SynMap()
    mapping = 'SynMap(True, {}, {})'
    
    [PROFILE]
    
    # How to weight each streamline (1D) or each node (2D)
    # when calculating the tract-profiles. If callable, this is a
    # function that calculates weights. If None, no weighting will
    # be applied. If "gauss", gaussian weights will be used.
    # If "median", the median of values at each node will be used
    # instead of a mean or weighted mean.
    # Default: "gauss"
    profile_weights = 'gauss'
    
    [BUNDLES]
    
    # List of bundle names to include in segmentation,
    # or a bundle dictionary (see make_bundle_dict for inspiration).
    # If None, will get all appropriate bundles for the chosen
    # segmentation algorithm.
    # Default: None
    bundle_info = ''
    
    # List of scalars to use.
    # Can be any of: "dti_fa", "dti_md", "dki_fa", "dki_md", "dki_awf",
    # "dki_mk". Can also be a scalar from AFQ.definitions.scalar.
    # Default:
    scalars = ['dti_fa', 'dti_md']
    
    [COMPUTE]
    
    # Whether to use a dask DataFrame object.
    # Default: False
    dask_it = false
    
    [VIZ]
    
    # Whether to use a virtual fram buffer. This is neccessary if
    # generating GIFs in a headless environment. Default: False
    virtual_frame_buffer = false
    
    # Which visualization backend to use.
    # See Visualization Backends page in documentation for details:
    # https://yeatmanlab.github.io/pyAFQ/usage/viz_backend.html
    # One of {"fury", "plotly", "plotly_no_gif"}.
    # Default: "plotly_no_gif"
    viz_backend = 'plotly_no_gif'
    
    [TRACTOGRAPHY]
    
    # How tracking directions are determined.
    # One of: {"det" | "prob"}
    directions = 'det'
    
    # The maximum turning angle in each step. Default: 30
    max_angle = 30.0
    
    # The discretization of direction getting. default:
    # dipy.data.default_sphere.
    sphere = ''
    
    # Float or binary mask describing the ROI within which we seed for
    # tracking.
    # Default to the entire volume (all ones).
    seed_mask = ''
    
    # A value of the seed_mask below which tracking is terminated.
    # Default to 0.
    seed_threshold = 0
    
    # The seeding density: if this is an int, it is is how many seeds in each
    # voxel on each dimension (for example, 2 => [2, 2, 2]). If this is a 2D
    # array, these are the coordinates of the seeds. Unless random_seeds is
    # set to True, in which case this is the total number of random seeds
    # to generate within the mask.
    n_seeds = 1
    
    # Whether to generate a total of n_seeds random seeds in the mask.
    # Default: False.
    random_seeds = false
    
    # random seed used to generate random seeds if random_seeds is
    # set to True. Default: None
    rng_seed = ''
    
    # If array: A float or binary mask that determines a stopping criterion
    # (e.g. FA).
    # If tuple: it contains a sequence that is interpreted as:
    # (pve_wm, pve_gm, pve_csf), each item of which is either a string
    # (full path) or a nibabel img to be used in particle filtering
    # tractography.
    # A tuple is required if tracker is set to "pft".
    # Defaults to no stopping (all ones).
    stop_mask = ''
    
    # If float, this a value of the stop_mask below which tracking is
    # terminated (and stop_mask has to be an array).
    # If str, "CMC" for Continuous Map Criterion [Girard2014]_.
    # "ACT" for Anatomically-constrained tractography [Smith2012]_.
    # A string is required if the tracker is set to "pft".
    # Defaults to 0 (this means that if no stop_mask is passed,
    # we will stop only at the edge of the image).
    stop_threshold = 0
    
    # The size (in mm) of a step of tractography. Default: 1.0
    step_size = 0.5
    
    # The miminal length (mm) in a streamline. Default: 10
    min_length = 10
    
    # The miminal length (mm) in a streamline. Default: 1000
    max_length = 1000
    
    # One of {"DTI", "CSD", "DKI", "MSMT"}. Defaults to use "DTI"
    odf_model = 'DTI'
    
    # Which strategy to use in tracking. This can be the standard local
    # tracking ("local") or Particle Filtering Tracking ([Girard2014]_).
    # One of {"local", "pft"}. Default: "local"
    tracker = 'local'
    
    [SEGMENTATION]
    
    # Resample streamlines to nb_points number of points.
    # If False, no resampling is done. Default: False
    nb_points = false
    
    # Subsample streamlines to nb_streamlines.
    # If False, no subsampling is don. Default: False
    nb_streamlines = false
    
    # Algorithm for segmentation (case-insensitive):
    # 'AFQ': Segment streamlines into bundles,
    # based on inclusion/exclusion ROIs.
    # 'Reco': Segment streamlines using the RecoBundles algorithm
    # [Garyfallidis2017].
    # Default: 'AFQ'
    seg_algo = 'AFQ'
    
    # Algorithm for streamline registration (case-insensitive):
    # 'slr' : Use Streamlinear Registration [Garyfallidis2015]_
    # 'syn' : Use image-based nonlinear registration
    # If None, will use SyN if a mapping is provided, slr otherwise.
    # If  seg_algo="AFQ", SyN is always used.
    # Default: None
    reg_algo = ''
    
    # Whether to clip the streamlines to be only in between the ROIs.
    # Default: False
    clip_edges = false
    
    # How to parallelize segmentation across processes when performing
    # waypoint ROI segmentation. Set to {"engine": "serial"} to not
    # perform parallelization. See ``AFQ.utils.parallel.pafor`` for
    # details.
    # Default: {"n_jobs": -1, "engine": "joblib",
    # "backend": "loky"}
    parallel_segmentation = "{'n_jobs': -1, 'engine': 'joblib', 'backend': 'loky'}"
    
    # Using RecoBundles Algorithm.
    # Whether or not to use progressive technique
    # during whole brain SLR.
    # Default: True.
    progressive = true
    
    # Using RecoBundles Algorithm.
    # Keep streamlines that have length greater than this value
    # during whole brain SLR.
    # Default: 50.
    greater_than = 50
    
    # Using RecoBundles Algorithm.
    # Remove clusters that have less than this value
    # during whole brain SLR.
    # Default: 50
    rm_small_clusters = 50
    
    # Parameter passed on to recognize for Recobundles.
    # See Recobundles documentation.
    # Default: 5
    model_clust_thr = 5
    
    # Parameter passed on to recognize for Recobundles.
    # See Recobundles documentation.
    # Default: 20
    reduction_thr = 20
    
    # Parameter passed on to recognize for Recobundles.
    # See Recobundles documentation.
    # Default: False
    refine = false
    
    # Parameter passed on to recognize for Recobundles.
    # See Recobundles documentation.
    # Default: 5
    pruning_thr = 5
    
    # Using AFQ Algorithm.
    # All b-values with values less than or equal to `bo_threshold` are
    # considered as b0s i.e. without diffusion weighting.
    # Default: 50.
    b0_threshold = 50
    
    # Using AFQ Algorithm.
    # Initial cleaning of fiber groups is done using probability maps
    # from [Hua2008]_. Here, we choose an average probability that
    # needs to be exceeded for an individual streamline to be retained.
    # Default: 0.
    prob_threshold = 0
    
    # The distance that a streamline node has to be from the waypoint
    # ROI in order to be included or excluded.
    # If set to None (default), will be calculated as the
    # center-to-corner distance of the voxel in the diffusion data.
    # If a bundle has additional_tolerance in its bundle_dict, that
    # tolerance will be added to this distance.
    # For example, if you wanted to increase tolerance for the right
    # arcuate waypoint ROIs by 3 each, you could make the following
    # modification to your bundle_dict:
    # bundle_dict["ARC_R"]["additional_tolerances"] = [3, 3]
    # Additional tolerances can also be negative.
    dist_to_waypoint = ''
    
    # If None, creates RandomState.
    # If int, creates RandomState with seed rng.
    # Used in RecoBundles Algorithm.
    # Default: None.
    rng = ''
    
    # Whether to return the indices in the original streamlines as part
    # of the output of segmentation.
    return_idx = false
    
    # If not None, presegment by ROIs before performing
    # RecoBundles. Only used if seg_algo starts with 'Reco'.
    # Meta-data for the segmentation. The format is something like::
    # {'name': {'ROIs':[img1, img2],
    # 'rules':[True, True]},
    # 'prob_map': img3,
    # 'cross_midline': False}
    # Default: None
    presegment_bundle_dict = ''
    
    # Optional arguments for initializing the segmentation for the
    # presegmentation. Only used if presegment_bundle_dict is not None.
    # Default: {}
    presegment_kawrgs = "{}"
    
    # Whether to filter the bundles based on their endpoints relative
    # to regions defined in the AAL atlas. Applies only to the waypoint
    # approach (XXX for now). Default: True.
    filter_by_endpoints = true
    
    # AAL atlas, which is the default behavior.
    # The format for this should be:
    # {"bundle1": {"startpoint":img1_1,
    # "endpoint":img1_2},
    # "bundle2": {"startpoint":img2_1,
    # "endpoint":img2_2}}
    # where the images used are binary masks of the desired
    # endpoints.
    endpoint_info = ''
    
    # If filter_by_endpoints is True, this is the required distance
    # from the endpoints to the atlas ROIs.
    dist_to_atlas = 4
    
    # The full path to a folder into which intermediate products
    # are saved. Default: None, means no saving of intermediates.
    save_intermediates = ''
    
    [CLEANING]
    
    # Number of points to resample streamlines to.
    # Default: 100
    n_points = 100
    
    # Number of rounds of cleaning based on the Mahalanobis distance from
    # the mean of extracted bundles. Default: 5
    clean_rounds = 5
    
    # Threshold of cleaning based on the Mahalanobis distance (the units are
    # standard deviations). Default: 5.
    distance_threshold = 5
    
    # Threshold for cleaning based on length (in standard deviations). Length
    # of any streamline should not be *more* than this number of stdevs from
    # the mean length.
    length_threshold = 4
    
    # Number of streamlines in a bundle under which we will
    # not bother with cleaning outliers. Default: 20.
    min_sl = 20
    
    # The statistic of each node relative to which the Mahalanobis is
    # calculated. Default: `np.mean` (but can also use median, etc.)
    stat = 'mean'
    
    # Whether to return indices in the original streamlines.
    # Default: False.
    return_idx = false
    
    
    
pyAFQ will store a copy of the configuration file alongside the computed
results. Note that the `title` variable and `[metadata]` section are both for
users to enter any title/metadata they would like and pyAFQ will generally
ignore them.
